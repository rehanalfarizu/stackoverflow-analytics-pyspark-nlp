{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eff65ac5",
   "metadata": {},
   "source": [
    "# Stack Overflow Analytics dengan PySpark dan NLP\n",
    "\n",
    "**UAS - Big Data Predictive Analytics Lanjut**\n",
    "\n",
    "Notebook ini mendemonstrasikan pipeline analisis data Stack Overflow menggunakan:\n",
    "- Apache Spark untuk Big Data Processing\n",
    "- NLP untuk Text Analysis\n",
    "- Machine Learning untuk Predictive Analytics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c546c765",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "Install dependencies yang diperlukan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18f6957a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "‚úÖ Dependencies installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install pyspark==3.5.0 pandas numpy nltk matplotlib seaborn wordcloud plotly kaleido -q\n",
    "\n",
    "print(\"‚úÖ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a20e577c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK data downloaded!\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK data\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "\n",
    "print(\"NLTK data downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caf6953a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'distutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m functions \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HashingTF, IDF, Tokenizer, StopWordsRemover\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclassification\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MulticlassClassificationEvaluator\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/final projek UAS/big-data-mining-/.venv/lib/python3.14/site-packages/pyspark/ml/__init__.py:31\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     23\u001b[39m     Estimator,\n\u001b[32m     24\u001b[39m     Model,\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m     UnaryTransformer,\n\u001b[32m     29\u001b[39m )\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pipeline, PipelineModel\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     32\u001b[39m     classification,\n\u001b[32m     33\u001b[39m     clustering,\n\u001b[32m     34\u001b[39m     evaluation,\n\u001b[32m     35\u001b[39m     feature,\n\u001b[32m     36\u001b[39m     fpm,\n\u001b[32m     37\u001b[39m     image,\n\u001b[32m     38\u001b[39m     recommendation,\n\u001b[32m     39\u001b[39m     regression,\n\u001b[32m     40\u001b[39m     stat,\n\u001b[32m     41\u001b[39m     tuning,\n\u001b[32m     42\u001b[39m     util,\n\u001b[32m     43\u001b[39m     linalg,\n\u001b[32m     44\u001b[39m     param,\n\u001b[32m     45\u001b[39m )\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TorchDistributor\n\u001b[32m     48\u001b[39m __all__ = [\n\u001b[32m     49\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTransformer\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     50\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mUnaryTransformer\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTorchDistributor\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     71\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/final projek UAS/big-data-mining-/.venv/lib/python3.14/site-packages/pyspark/ml/image.py:31\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, List, NoReturn, Optional, cast\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdistutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LooseVersion\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkContext\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Row, StructType, _create_row, _parse_datatype_json_string\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'distutils'"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Setup untuk Google Colab - Chart Generator\n",
    "try:\n",
    "    # Jika di Colab, clone repo atau import langsung\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"üîµ Running in Google Colab\")\n",
    "    \n",
    "    # Setup inline plotting\n",
    "    %matplotlib inline\n",
    "    from IPython.display import set_matplotlib_formats\n",
    "    try:\n",
    "        set_matplotlib_formats('retina')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    plt.rcParams['figure.dpi'] = 100\n",
    "    plt.rcParams['savefig.dpi'] = 150\n",
    "    \n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"üîµ Running locally\")\n",
    "\n",
    "# Import Plotly untuk interactive charts\n",
    "try:\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    PLOTLY_AVAILABLE = True\n",
    "    print(\"‚úÖ Plotly available for interactive charts\")\n",
    "except ImportError:\n",
    "    PLOTLY_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è Plotly not available, using static charts\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a17e864",
   "metadata": {},
   "source": [
    "## 2. Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ac4536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StackOverflow-Analytics\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(\"Spark Session created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7511f70d",
   "metadata": {},
   "source": [
    "## 3. Create Sample Dataset\n",
    "\n",
    "Membuat sample data yang merepresentasikan Stack Overflow questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5862e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Stack Overflow Data\n",
    "sample_data = [\n",
    "    (1, \"How to read CSV file in Python using pandas?\", \n",
    "     \"I want to read a CSV file and convert it to DataFrame. What is the best approach?\",\n",
    "     \"<python><pandas><csv>\", 45, 12500, 5, 1, \"2024-01-15\"),\n",
    "    (2, \"JavaScript async await not working properly\",\n",
    "     \"My async function is not waiting for the promise to resolve. Getting undefined.\",\n",
    "     \"<javascript><async-await><promise>\", 32, 8900, 4, 1, \"2024-01-20\"),\n",
    "    (3, \"Docker container networking between containers\",\n",
    "     \"How can I connect two Docker containers so they can communicate with each other?\",\n",
    "     \"<docker><networking><containers>\", 28, 6700, 3, 1, \"2024-02-05\"),\n",
    "    (4, \"Python list comprehension with if else\",\n",
    "     \"How do I write a list comprehension with conditional logic in Python?\",\n",
    "     \"<python><list-comprehension>\", 55, 15000, 6, 1, \"2024-02-10\"),\n",
    "    (5, \"React useState not updating immediately\",\n",
    "     \"State updates are not reflected immediately after calling setState. Why?\",\n",
    "     \"<reactjs><hooks><state>\", 38, 11200, 5, 1, \"2024-02-15\"),\n",
    "    (6, \"SQL JOIN multiple tables with conditions\",\n",
    "     \"How to join three tables with WHERE clause and GROUP BY in SQL?\",\n",
    "     \"<sql><join><mysql>\", 42, 9800, 4, 1, \"2024-02-20\"),\n",
    "    (7, \"Git merge conflict resolution best practices\",\n",
    "     \"What is the best way to resolve merge conflicts in Git without losing changes?\",\n",
    "     \"<git><merge-conflict><version-control>\", 35, 7500, 4, 1, \"2024-03-01\"),\n",
    "    (8, \"Machine learning model overfitting problem\",\n",
    "     \"My neural network is overfitting on training data. How to prevent this?\",\n",
    "     \"<machine-learning><overfitting><deep-learning>\", 48, 13400, 6, 1, \"2024-03-05\"),\n",
    "    (9, \"Kubernetes pod keeps crashing with OOMKilled\",\n",
    "     \"My Kubernetes pod is being killed due to memory limit. How to debug?\",\n",
    "     \"<kubernetes><docker><memory>\", 25, 5600, 3, 1, \"2024-03-10\"),\n",
    "    (10, \"Python pandas merge dataframes on multiple columns\",\n",
    "     \"How to merge two dataframes on multiple columns with different names?\",\n",
    "     \"<python><pandas><dataframe>\", 52, 14200, 5, 1, \"2024-03-15\"),\n",
    "    (11, \"CSS flexbox center align not working\",\n",
    "     \"I am trying to center a div using flexbox but it is not centering properly.\",\n",
    "     \"<css><flexbox><html>\", 22, 4800, 3, 0, \"2024-03-20\"),\n",
    "    (12, \"Node.js express middleware order matters\",\n",
    "     \"Why does the order of middleware in Express.js affect my application?\",\n",
    "     \"<node.js><express><middleware>\", 30, 6200, 3, 1, \"2024-03-25\"),\n",
    "    (13, \"TensorFlow GPU not detected on Windows\",\n",
    "     \"TensorFlow is not recognizing my NVIDIA GPU. CUDA is installed properly.\",\n",
    "     \"<tensorflow><gpu><cuda>\", 18, 4200, 2, 0, \"2024-04-01\"),\n",
    "    (14, \"Apache Spark DataFrame operations slow\",\n",
    "     \"My Spark job is running very slow. How to optimize DataFrame operations?\",\n",
    "     \"<apache-spark><pyspark><performance>\", 40, 8500, 4, 1, \"2024-04-05\"),\n",
    "    (15, \"REST API authentication JWT vs OAuth\",\n",
    "     \"What is the difference between JWT and OAuth for API authentication?\",\n",
    "     \"<rest><authentication><jwt><oauth>\", 65, 18000, 7, 1, \"2024-04-10\"),\n",
    "    (16, \"Why is my Python code so slow?\",\n",
    "     \"My Python script takes too long to run. How can I make it faster?\",\n",
    "     \"<python><performance><optimization>\", -2, 1200, 1, 0, \"2024-04-15\"),\n",
    "    (17, \"Help needed with homework assignment\",\n",
    "     \"Can someone solve this for me? I don't understand the question.\",\n",
    "     \"<python>\", -5, 800, 0, 0, \"2024-04-20\"),\n",
    "    (18, \"Best programming language to learn in 2024\",\n",
    "     \"Which programming language should I learn as a beginner in 2024?\",\n",
    "     \"<programming-languages><career>\", 15, 5500, 8, 0, \"2024-04-25\"),\n",
    "    (19, \"MongoDB aggregation pipeline group by date\",\n",
    "     \"How to group documents by date and calculate sum in MongoDB aggregation?\",\n",
    "     \"<mongodb><aggregation><database>\", 33, 7200, 3, 1, \"2024-05-01\"),\n",
    "    (20, \"AWS Lambda cold start optimization\",\n",
    "     \"How to reduce cold start time for AWS Lambda functions in Python?\",\n",
    "     \"<aws-lambda><python><serverless>\", 45, 9600, 4, 1, \"2024-05-05\"),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"Id\", IntegerType(), True),\n",
    "    StructField(\"Title\", StringType(), True),\n",
    "    StructField(\"Body\", StringType(), True),\n",
    "    StructField(\"Tags\", StringType(), True),\n",
    "    StructField(\"Score\", IntegerType(), True),\n",
    "    StructField(\"ViewCount\", IntegerType(), True),\n",
    "    StructField(\"AnswerCount\", IntegerType(), True),\n",
    "    StructField(\"HasAcceptedAnswer\", IntegerType(), True),\n",
    "    StructField(\"CreationDate\", StringType(), True),\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(sample_data, schema)\n",
    "\n",
    "print(f\"Total Questions: {df.count()}\")\n",
    "df.show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6895c85",
   "metadata": {},
   "source": [
    "## 4. ETL - Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade843c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF untuk parsing tags\n",
    "@F.udf(ArrayType(StringType()))\n",
    "def parse_tags(tags_str):\n",
    "    if not tags_str:\n",
    "        return []\n",
    "    return re.findall(r'<([^>]+)>', tags_str)\n",
    "\n",
    "# Transform data\n",
    "df_transformed = df \\\n",
    "    .withColumn(\"TagsList\", parse_tags(F.col(\"Tags\"))) \\\n",
    "    .withColumn(\"TagCount\", F.size(\"TagsList\")) \\\n",
    "    .withColumn(\"TitleLength\", F.length(\"Title\")) \\\n",
    "    .withColumn(\"BodyLength\", F.length(\"Body\")) \\\n",
    "    .withColumn(\"TitleWordCount\", F.size(F.split(\"Title\", \" \"))) \\\n",
    "    .withColumn(\"CreationDate\", F.to_date(\"CreationDate\")) \\\n",
    "    .withColumn(\"Year\", F.year(\"CreationDate\")) \\\n",
    "    .withColumn(\"Month\", F.month(\"CreationDate\"))\n",
    "\n",
    "# Add quality label\n",
    "df_transformed = df_transformed.withColumn(\n",
    "    \"QualityLabel\",\n",
    "    F.when((F.col(\"Score\") >= 30) & (F.col(\"HasAcceptedAnswer\") == 1), 2)  # High\n",
    "    .when(F.col(\"Score\") >= 0, 1)  # Medium\n",
    "    .otherwise(0)  # Low\n",
    ")\n",
    "\n",
    "print(\"Transformed Data:\")\n",
    "df_transformed.select(\"Title\", \"Score\", \"TagCount\", \"TitleLength\", \"QualityLabel\").show(10, truncate=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3402b7d7",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7fa29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas for visualization\n",
    "pdf = df_transformed.toPandas()\n",
    "\n",
    "# Summary Statistics\n",
    "print(\"=\" * 50)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total Questions: {len(pdf)}\")\n",
    "print(f\"Average Score: {pdf['Score'].mean():.2f}\")\n",
    "print(f\"Average Views: {pdf['ViewCount'].mean():.0f}\")\n",
    "print(f\"Questions with Accepted Answer: {pdf['HasAcceptedAnswer'].sum()}\")\n",
    "print(f\"Average Tags per Question: {pdf['TagCount'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1268f220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper class untuk chart (compatible with Colab)\n",
    "class ColabChartGenerator:\n",
    "    \"\"\"Chart Generator yang kompatibel dengan Google Colab.\"\"\"\n",
    "    \n",
    "    def __init__(self, figsize=(12, 6), interactive=None):\n",
    "        self.figsize = figsize\n",
    "        self.colors = sns.color_palette(\"husl\", 10)\n",
    "        self.hex_colors = ['#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231',\n",
    "                          '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe']\n",
    "        \n",
    "        # Auto-detect: use interactive in Colab if Plotly available\n",
    "        if interactive is None:\n",
    "            self.interactive = IN_COLAB and PLOTLY_AVAILABLE\n",
    "        else:\n",
    "            self.interactive = interactive and PLOTLY_AVAILABLE\n",
    "    \n",
    "    def plot_bar(self, df, x_column, y_column, title=\"Bar Chart\", \n",
    "                 horizontal=False, top_n=None, color=None):\n",
    "        \"\"\"Plot bar chart - interactive di Colab.\"\"\"\n",
    "        plot_df = df.copy()\n",
    "        if top_n:\n",
    "            plot_df = plot_df.nlargest(top_n, y_column)\n",
    "        \n",
    "        if self.interactive:\n",
    "            if horizontal:\n",
    "                fig = px.bar(plot_df, x=y_column, y=x_column, orientation='h',\n",
    "                           title=title, color_discrete_sequence=self.hex_colors)\n",
    "            else:\n",
    "                fig = px.bar(plot_df, x=x_column, y=y_column,\n",
    "                           title=title, color_discrete_sequence=self.hex_colors)\n",
    "            fig.update_layout(template='plotly_white')\n",
    "            return fig\n",
    "        else:\n",
    "            fig, ax = plt.subplots(figsize=self.figsize)\n",
    "            if horizontal:\n",
    "                ax.barh(plot_df[x_column], plot_df[y_column], color=color or 'steelblue')\n",
    "            else:\n",
    "                ax.bar(plot_df[x_column], plot_df[y_column], color=color or 'steelblue')\n",
    "                plt.xticks(rotation=45, ha='right')\n",
    "            ax.set_title(title)\n",
    "            plt.tight_layout()\n",
    "            return fig\n",
    "    \n",
    "    def plot_pie(self, df, values_column, labels_column, title=\"Distribution\", colors=None):\n",
    "        \"\"\"Plot pie chart.\"\"\"\n",
    "        if self.interactive:\n",
    "            fig = px.pie(df, values=values_column, names=labels_column, title=title)\n",
    "            fig.update_traces(textposition='inside', textinfo='percent+label')\n",
    "            fig.update_layout(template='plotly_white')\n",
    "            return fig\n",
    "        else:\n",
    "            fig, ax = plt.subplots(figsize=(8, 8))\n",
    "            ax.pie(df[values_column], labels=df[labels_column], \n",
    "                   autopct='%1.1f%%', colors=colors)\n",
    "            ax.set_title(title)\n",
    "            return fig\n",
    "    \n",
    "    def plot_scatter(self, df, x_column, y_column, title=\"Scatter Plot\",\n",
    "                    color_column=None, size_column=None):\n",
    "        \"\"\"Plot scatter chart - great for Colab interactivity.\"\"\"\n",
    "        if self.interactive:\n",
    "            fig = px.scatter(df, x=x_column, y=y_column, color=color_column,\n",
    "                           size=size_column, title=title)\n",
    "            fig.update_layout(template='plotly_white')\n",
    "            return fig\n",
    "        else:\n",
    "            fig, ax = plt.subplots(figsize=self.figsize)\n",
    "            ax.scatter(df[x_column], df[y_column], alpha=0.7, c='steelblue')\n",
    "            ax.set_xlabel(x_column)\n",
    "            ax.set_ylabel(y_column)\n",
    "            ax.set_title(title)\n",
    "            plt.tight_layout()\n",
    "            return fig\n",
    "    \n",
    "    def plot_histogram(self, data, title=\"Distribution\", bins=30, kde=True):\n",
    "        \"\"\"Plot histogram.\"\"\"\n",
    "        if self.interactive:\n",
    "            fig = px.histogram(pd.DataFrame({'value': data}), x='value', \n",
    "                             nbins=bins, title=title, marginal='box')\n",
    "            fig.update_layout(template='plotly_white')\n",
    "            return fig\n",
    "        else:\n",
    "            fig, ax = plt.subplots(figsize=self.figsize)\n",
    "            sns.histplot(data, bins=bins, kde=kde, ax=ax, color='steelblue')\n",
    "            ax.set_title(title)\n",
    "            plt.tight_layout()\n",
    "            return fig\n",
    "    \n",
    "    def plot_heatmap(self, data, title=\"Heatmap\", cmap='YlOrRd'):\n",
    "        \"\"\"Plot heatmap.\"\"\"\n",
    "        if self.interactive:\n",
    "            fig = px.imshow(data, title=title, text_auto=True)\n",
    "            fig.update_layout(template='plotly_white')\n",
    "            return fig\n",
    "        else:\n",
    "            fig, ax = plt.subplots(figsize=self.figsize)\n",
    "            sns.heatmap(data, annot=True, fmt='.1f', cmap=cmap, ax=ax)\n",
    "            ax.set_title(title)\n",
    "            plt.tight_layout()\n",
    "            return fig\n",
    "    \n",
    "    def plot_line(self, df, x_column, y_column, title=\"Line Chart\", \n",
    "                  group_column=None):\n",
    "        \"\"\"Plot line chart.\"\"\"\n",
    "        if self.interactive:\n",
    "            if group_column:\n",
    "                fig = px.line(df, x=x_column, y=y_column, color=group_column, title=title)\n",
    "            else:\n",
    "                fig = px.line(df, x=x_column, y=y_column, title=title)\n",
    "            fig.update_layout(template='plotly_white')\n",
    "            return fig\n",
    "        else:\n",
    "            fig, ax = plt.subplots(figsize=self.figsize)\n",
    "            if group_column:\n",
    "                for group in df[group_column].unique():\n",
    "                    group_df = df[df[group_column] == group]\n",
    "                    ax.plot(group_df[x_column], group_df[y_column], label=group)\n",
    "                ax.legend()\n",
    "            else:\n",
    "                ax.plot(df[x_column], df[y_column])\n",
    "            ax.set_title(title)\n",
    "            ax.set_xlabel(x_column)\n",
    "            ax.set_ylabel(y_column)\n",
    "            plt.tight_layout()\n",
    "            return fig\n",
    "    \n",
    "    def show(self, fig):\n",
    "        \"\"\"Display figure.\"\"\"\n",
    "        if hasattr(fig, 'show'):  # Plotly\n",
    "            fig.show()\n",
    "        else:  # Matplotlib\n",
    "            plt.show()\n",
    "\n",
    "# Initialize chart generator\n",
    "charts = ColabChartGenerator(interactive=PLOTLY_AVAILABLE)\n",
    "print(f\"‚úÖ Chart Generator initialized (interactive={charts.interactive})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d909cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Visualizations (works great in Colab!)\n",
    "\n",
    "# 1. Score Distribution - Interactive Histogram\n",
    "fig1 = charts.plot_histogram(pdf['Score'], title='üìä Distribution of Question Scores', bins=15)\n",
    "charts.show(fig1)\n",
    "\n",
    "# 2. Quality Distribution - Interactive Pie\n",
    "quality_df = pdf.groupby('QualityLabel').size().reset_index(name='count')\n",
    "quality_df['Quality'] = quality_df['QualityLabel'].map({0: 'Low', 1: 'Medium', 2: 'High'})\n",
    "fig2 = charts.plot_pie(quality_df, values_column='count', labels_column='Quality', \n",
    "                       title='üìà Question Quality Distribution')\n",
    "charts.show(fig2)\n",
    "\n",
    "# 3. Score vs Views - Interactive Scatter (hover untuk lihat detail!)\n",
    "fig3 = charts.plot_scatter(pdf, x_column='Score', y_column='ViewCount', \n",
    "                          title='üîç Score vs View Count (Hover for details!)')\n",
    "charts.show(fig3)\n",
    "\n",
    "# 4. Questions by Month - Interactive Bar\n",
    "monthly_df = pdf.groupby('Month').size().reset_index(name='Count')\n",
    "fig4 = charts.plot_bar(monthly_df, x_column='Month', y_column='Count', \n",
    "                       title='üìÖ Questions by Month')\n",
    "charts.show(fig4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5a2897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag Analysis\n",
    "from collections import Counter\n",
    "\n",
    "# Flatten all tags\n",
    "all_tags = [tag for tags in pdf['TagsList'] for tag in tags]\n",
    "tag_counts = Counter(all_tags)\n",
    "\n",
    "print(\"Top 10 Most Popular Tags:\")\n",
    "print(\"-\" * 30)\n",
    "for tag, count in tag_counts.most_common(10):\n",
    "    print(f\"{tag}: {count}\")\n",
    "\n",
    "# Tag WordCloud\n",
    "wordcloud = WordCloud(\n",
    "    width=800, height=400,\n",
    "    background_color='white',\n",
    "    colormap='viridis'\n",
    ").generate_from_frequencies(tag_counts)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Stack Overflow Tags Word Cloud', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0464d5a4",
   "metadata": {},
   "source": [
    "## 6. NLP - Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6a5108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Title and Body for text analysis\n",
    "df_text = df_transformed.withColumn(\n",
    "    \"CombinedText\",\n",
    "    F.concat_ws(\" \", F.col(\"Title\"), F.col(\"Body\"))\n",
    ")\n",
    "\n",
    "# Clean text\n",
    "@F.udf(StringType())\n",
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df_text = df_text.withColumn(\"CleanText\", clean_text(F.col(\"CombinedText\")))\n",
    "\n",
    "print(\"Cleaned Text Sample:\")\n",
    "df_text.select(\"Title\", \"CleanText\").show(3, truncate=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ad127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenizer = Tokenizer(inputCol=\"CleanText\", outputCol=\"Words\")\n",
    "df_tokenized = tokenizer.transform(df_text)\n",
    "\n",
    "# Remove Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = list(stopwords.words('english'))\n",
    "# Add custom tech stopwords\n",
    "stop_words.extend(['want', 'using', 'use', 'get', 'would', 'like', 'need', 'trying'])\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"Words\", outputCol=\"FilteredWords\", stopWords=stop_words)\n",
    "df_filtered = remover.transform(df_tokenized)\n",
    "\n",
    "print(\"Tokenized and Filtered Words:\")\n",
    "df_filtered.select(\"Title\", \"FilteredWords\").show(3, truncate=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976f6295",
   "metadata": {},
   "source": [
    "## 7. NLP - Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f1b53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize VADER\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Custom tech lexicon\n",
    "tech_lexicon = {\n",
    "    'solved': 2.0, 'works': 1.5, 'working': 1.0, 'fixed': 2.0, 'perfect': 2.5,\n",
    "    'error': -2.0, 'bug': -1.5, 'broken': -2.0, 'crash': -2.5, 'fail': -2.0,\n",
    "    'slow': -1.5, 'deprecated': -1.0, 'issue': -1.0, 'problem': -1.0\n",
    "}\n",
    "sia.lexicon.update(tech_lexicon)\n",
    "\n",
    "# UDF for sentiment\n",
    "@F.udf(StringType())\n",
    "def get_sentiment(text):\n",
    "    if not text:\n",
    "        return \"neutral\"\n",
    "    scores = sia.polarity_scores(text)\n",
    "    compound = scores['compound']\n",
    "    if compound >= 0.05:\n",
    "        return \"positive\"\n",
    "    elif compound <= -0.05:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "@F.udf(FloatType())\n",
    "def get_sentiment_score(text):\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    return float(sia.polarity_scores(text)['compound'])\n",
    "\n",
    "# Apply sentiment analysis\n",
    "df_sentiment = df_filtered \\\n",
    "    .withColumn(\"Sentiment\", get_sentiment(F.col(\"CombinedText\"))) \\\n",
    "    .withColumn(\"SentimentScore\", get_sentiment_score(F.col(\"CombinedText\")))\n",
    "\n",
    "print(\"Sentiment Analysis Results:\")\n",
    "df_sentiment.select(\"Title\", \"Sentiment\", \"SentimentScore\").show(10, truncate=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4da1c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Distribution - Interactive Bar Chart\n",
    "sentiment_counts = df_sentiment.groupBy(\"Sentiment\").count().toPandas()\n",
    "\n",
    "# Using our Colab-compatible chart generator\n",
    "fig = charts.plot_bar(sentiment_counts, x_column='Sentiment', y_column='count',\n",
    "                     title='üé≠ Question Sentiment Distribution')\n",
    "charts.show(fig)\n",
    "\n",
    "# Sentiment vs Score - Interactive Scatter\n",
    "pdf_sentiment = df_sentiment.select(\"Title\", \"Sentiment\", \"SentimentScore\", \"Score\").toPandas()\n",
    "\n",
    "if charts.interactive:\n",
    "    fig2 = px.scatter(pdf_sentiment, x='SentimentScore', y='Score', \n",
    "                      color='Sentiment', hover_data=['Title'],\n",
    "                      title='üí° Sentiment Score vs Question Score (Hover to see question!)',\n",
    "                      color_discrete_map={'positive': '#6bcb77', 'neutral': '#4d96ff', 'negative': '#ff6b6b'})\n",
    "    fig2.update_layout(template='plotly_white')\n",
    "    fig2.show()\n",
    "else:\n",
    "    colors = {'positive': '#6bcb77', 'neutral': '#4d96ff', 'negative': '#ff6b6b'}\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for sentiment in pdf_sentiment['Sentiment'].unique():\n",
    "        data = pdf_sentiment[pdf_sentiment['Sentiment'] == sentiment]\n",
    "        plt.scatter(data['SentimentScore'], data['Score'], \n",
    "                   label=sentiment, color=colors.get(sentiment, 'gray'), alpha=0.7)\n",
    "    plt.xlabel('Sentiment Score')\n",
    "    plt.ylabel('Question Score')\n",
    "    plt.title('Sentiment Score vs Question Score')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6be91a",
   "metadata": {},
   "source": [
    "## 8. NLP - TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8eb9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "hashingTF = HashingTF(inputCol=\"FilteredWords\", outputCol=\"RawFeatures\", numFeatures=1000)\n",
    "df_tf = hashingTF.transform(df_sentiment)\n",
    "\n",
    "idf = IDF(inputCol=\"RawFeatures\", outputCol=\"TFIDFFeatures\")\n",
    "idf_model = idf.fit(df_tf)\n",
    "df_tfidf = idf_model.transform(df_tf)\n",
    "\n",
    "print(\"TF-IDF Features created successfully!\")\n",
    "df_tfidf.select(\"Title\", \"TFIDFFeatures\").show(3, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d4beaf",
   "metadata": {},
   "source": [
    "## 9. Machine Learning - Quality Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c42c1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Prepare features\n",
    "df_ml = df_tfidf.withColumn(\"label\", F.col(\"QualityLabel\").cast(\"double\"))\n",
    "\n",
    "# Numeric features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"TitleLength\", \"BodyLength\", \"TagCount\", \"TitleWordCount\", \"SentimentScore\"],\n",
    "    outputCol=\"NumericFeatures\"\n",
    ")\n",
    "df_ml = assembler.transform(df_ml)\n",
    "\n",
    "# Combine with TF-IDF\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "final_assembler = VectorAssembler(\n",
    "    inputCols=[\"NumericFeatures\", \"TFIDFFeatures\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "df_final = final_assembler.transform(df_ml)\n",
    "\n",
    "print(\"Features prepared for ML!\")\n",
    "print(f\"Total samples: {df_final.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18df96c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_df, test_df = df_final.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Training samples: {train_df.count()}\")\n",
    "print(f\"Testing samples: {test_df.count()}\")\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    numTrees=50,\n",
    "    maxDepth=10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "rf_model = rf.fit(train_df)\n",
    "print(\"Random Forest model trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5ac61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "predictions = rf_model.transform(test_df)\n",
    "\n",
    "# Evaluate\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "# F1 Score\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "f1 = evaluator_f1.evaluate(predictions)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"MODEL EVALUATION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bb22c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show predictions\n",
    "print(\"Sample Predictions:\")\n",
    "predictions.select(\"Title\", \"label\", \"prediction\", \"Score\").show(10, truncate=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af6b268",
   "metadata": {},
   "source": [
    "## 10. Trend Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9acaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag Trend Analysis\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "# Explode tags\n",
    "df_tags = df_transformed.select(\n",
    "    \"Id\", \"Year\", \"Month\", \"Score\", \"ViewCount\",\n",
    "    explode(\"TagsList\").alias(\"Tag\")\n",
    ")\n",
    "\n",
    "# Tag statistics\n",
    "tag_stats = df_tags.groupBy(\"Tag\").agg(\n",
    "    F.count(\"Id\").alias(\"QuestionCount\"),\n",
    "    F.avg(\"Score\").alias(\"AvgScore\"),\n",
    "    F.sum(\"ViewCount\").alias(\"TotalViews\")\n",
    ").orderBy(F.desc(\"QuestionCount\"))\n",
    "\n",
    "print(\"Tag Statistics:\")\n",
    "tag_stats.show(15, truncate=False)\n",
    "\n",
    "# Interactive Tag Visualization\n",
    "tag_stats_pdf = tag_stats.toPandas()\n",
    "\n",
    "# Top 10 Tags - Interactive Bar\n",
    "fig = charts.plot_bar(tag_stats_pdf.head(10), x_column='Tag', y_column='QuestionCount',\n",
    "                     title='üè∑Ô∏è Top 10 Most Popular Tags')\n",
    "charts.show(fig)\n",
    "\n",
    "# Tag Score vs Views - Interactive Scatter (Bubble Chart)\n",
    "if charts.interactive:\n",
    "    fig2 = px.scatter(tag_stats_pdf, x='AvgScore', y='TotalViews', \n",
    "                     size='QuestionCount', hover_name='Tag',\n",
    "                     title='üéØ Tag Performance (Size = Question Count, Hover for Tag!)',\n",
    "                     color='QuestionCount', color_continuous_scale='viridis')\n",
    "    fig2.update_layout(template='plotly_white')\n",
    "    fig2.show()\n",
    "else:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(tag_stats_pdf['AvgScore'], tag_stats_pdf['TotalViews'], \n",
    "               s=tag_stats_pdf['QuestionCount']*20, alpha=0.6, c='steelblue')\n",
    "    for i, row in tag_stats_pdf.iterrows():\n",
    "        plt.annotate(row['Tag'], (row['AvgScore'], row['TotalViews']), fontsize=8)\n",
    "    plt.xlabel('Average Score')\n",
    "    plt.ylabel('Total Views')\n",
    "    plt.title('Tag Performance (Size = Question Count)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f49e28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly Trend - Interactive Dual Axis Chart\n",
    "monthly_trend = df_transformed.groupBy(\"Year\", \"Month\").agg(\n",
    "    F.count(\"Id\").alias(\"QuestionCount\"),\n",
    "    F.avg(\"Score\").alias(\"AvgScore\")\n",
    ").orderBy(\"Year\", \"Month\").toPandas()\n",
    "\n",
    "monthly_trend['Period'] = monthly_trend['Year'].astype(str) + '-' + monthly_trend['Month'].astype(str).str.zfill(2)\n",
    "\n",
    "if charts.interactive:\n",
    "    from plotly.subplots import make_subplots\n",
    "    \n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(x=monthly_trend['Period'], y=monthly_trend['QuestionCount'], \n",
    "               name=\"Question Count\", marker_color='steelblue', opacity=0.7),\n",
    "        secondary_y=False,\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=monthly_trend['Period'], y=monthly_trend['AvgScore'], \n",
    "                   name=\"Avg Score\", line=dict(color='red', width=3), mode='lines+markers'),\n",
    "        secondary_y=True,\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_text=\"üìà Monthly Question Trend (Interactive!)\",\n",
    "        template='plotly_white',\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    fig.update_xaxes(title_text=\"Period\")\n",
    "    fig.update_yaxes(title_text=\"Question Count\", secondary_y=False, color='steelblue')\n",
    "    fig.update_yaxes(title_text=\"Average Score\", secondary_y=True, color='red')\n",
    "    \n",
    "    fig.show()\n",
    "else:\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 5))\n",
    "    \n",
    "    ax1.bar(monthly_trend['Period'], monthly_trend['QuestionCount'], color='steelblue', alpha=0.7)\n",
    "    ax1.set_xlabel('Period')\n",
    "    ax1.set_ylabel('Question Count', color='steelblue')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(monthly_trend['Period'], monthly_trend['AvgScore'], color='red', marker='o', linewidth=2)\n",
    "    ax2.set_ylabel('Average Score', color='red')\n",
    "    \n",
    "    plt.title('Monthly Question Trend', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013697cd",
   "metadata": {},
   "source": [
    "## 11. Summary & Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ffeea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"STACK OVERFLOW ANALYTICS - SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"1. DATA OVERVIEW\")\n",
    "print(f\"   - Total Questions Analyzed: {df.count()}\")\n",
    "print(f\"   - Date Range: {pdf['CreationDate'].min()} to {pdf['CreationDate'].max()}\")\n",
    "print(f\"   - Unique Tags: {len(tag_counts)}\")\n",
    "print()\n",
    "print(\"2. TOP TECHNOLOGIES\")\n",
    "for i, (tag, count) in enumerate(tag_counts.most_common(5), 1):\n",
    "    print(f\"   {i}. {tag}: {count} questions\")\n",
    "print()\n",
    "print(\"3. QUALITY METRICS\")\n",
    "print(f\"   - High Quality Questions: {(pdf['QualityLabel'] == 2).sum()}\")\n",
    "print(f\"   - Medium Quality Questions: {(pdf['QualityLabel'] == 1).sum()}\")\n",
    "print(f\"   - Low Quality Questions: {(pdf['QualityLabel'] == 0).sum()}\")\n",
    "print()\n",
    "print(\"4. ML MODEL PERFORMANCE\")\n",
    "print(f\"   - Algorithm: Random Forest\")\n",
    "print(f\"   - Accuracy: {accuracy:.2%}\")\n",
    "print(f\"   - F1 Score: {f1:.2%}\")\n",
    "print()\n",
    "print(\"5. KEY INSIGHTS\")\n",
    "print(\"   - Python remains the most discussed technology\")\n",
    "print(\"   - Questions with more tags tend to get higher engagement\")\n",
    "print(\"   - Sentiment analysis helps identify problematic questions\")\n",
    "print()\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8a5802",
   "metadata": {},
   "source": [
    "## üì± Google Colab Tips\n",
    "\n",
    "Beberapa tips untuk menggunakan notebook ini di Google Colab:\n",
    "\n",
    "### Interactive Charts\n",
    "- Semua chart di notebook ini sudah **interactive** ketika dijalankan di Colab\n",
    "- Hover di atas data point untuk melihat detail\n",
    "- Zoom in/out dengan scroll\n",
    "- Pan dengan drag\n",
    "- Double-click untuk reset view\n",
    "\n",
    "### Menyimpan ke Google Drive\n",
    "Jalankan cell di bawah untuk menyimpan hasil analisis ke Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53af97f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpan hasil ke Google Drive (khusus Colab)\n",
    "def save_results_to_drive():\n",
    "    \"\"\"Simpan semua hasil analisis ke Google Drive.\"\"\"\n",
    "    if not IN_COLAB:\n",
    "        print(\"‚ùå Function ini hanya tersedia di Google Colab\")\n",
    "        return\n",
    "    \n",
    "    from google.colab import drive\n",
    "    import os\n",
    "    \n",
    "    # Mount Drive\n",
    "    if not os.path.exists('/content/drive'):\n",
    "        drive.mount('/content/drive')\n",
    "    \n",
    "    # Buat folder\n",
    "    save_dir = '/content/drive/MyDrive/stackoverflow_analytics'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Save DataFrames\n",
    "    pdf.to_csv(f'{save_dir}/questions_analyzed.csv', index=False)\n",
    "    tag_stats_pdf.to_csv(f'{save_dir}/tag_statistics.csv', index=False)\n",
    "    monthly_trend.to_csv(f'{save_dir}/monthly_trend.csv', index=False)\n",
    "    \n",
    "    # Save summary\n",
    "    summary = f\"\"\"\n",
    "Stack Overflow Analytics Summary\n",
    "================================\n",
    "Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "Total Questions: {len(pdf)}\n",
    "Unique Tags: {len(tag_stats_pdf)}\n",
    "Date Range: {pdf['CreationDate'].min()} to {pdf['CreationDate'].max()}\n",
    "\n",
    "Model Performance:\n",
    "- Accuracy: {accuracy:.4f}\n",
    "- F1 Score: {f1:.4f}\n",
    "\n",
    "Top 5 Tags:\n",
    "{tag_stats_pdf.head(5)[['Tag', 'QuestionCount']].to_string()}\n",
    "\"\"\"\n",
    "    \n",
    "    with open(f'{save_dir}/summary.txt', 'w') as f:\n",
    "        f.write(summary)\n",
    "    \n",
    "    print(f\"‚úÖ Results saved to: {save_dir}\")\n",
    "    print(\"   - questions_analyzed.csv\")\n",
    "    print(\"   - tag_statistics.csv\")\n",
    "    print(\"   - monthly_trend.csv\")\n",
    "    print(\"   - summary.txt\")\n",
    "\n",
    "# Uncomment untuk menyimpan:\n",
    "# save_results_to_drive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c706460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark Session\n",
    "spark.stop()\n",
    "print(\"Spark session stopped. Analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Big Data Mining)",
   "language": "python",
   "name": "bigdata-mining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
